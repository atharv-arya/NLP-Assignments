{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqkdKB4cVZub",
        "outputId": "a096eabc-9f8e-4b27-f120-94575580238d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRuwwGP_Vaca"
      },
      "source": [
        "# 1. Tokenization (15 pts)\n",
        "\n",
        "How does one represent textual input to language models? One strategy that we have seen is to split up words on spaces, e.g.,\n",
        "\n",
        "> This is an example.\n",
        "\n",
        "> [This, is, an, example],\n",
        "\n",
        "but this fails when unseen words appear at test time, e.g.,\n",
        "\n",
        "> We named our son nwonkun.\n",
        "\n",
        "> [We, named, our, son, \\<unk\\>] (5 tokens).\n",
        "\n",
        "One solution to this problem is to use character-level tokens\n",
        "\n",
        "> [W, e, _, n, a, m, e, d, _, o, u, r, _, s, o, n, _, n, w, o, n, k, u, n]\n",
        "\n",
        "(24 tokens, if I counted right), but now the number of tokens required to encode a sentence has increased a *lot*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr-8Wi_n0HUf"
      },
      "source": [
        "## 1.1 Byte-pair encodings and sub-word tokenization (5 pts)\n",
        "\n",
        "[Byte-pair encodings (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding) are a clever middle ground for the tokenization problem.\n",
        "Starting from a character-level tokenization, iteratively combine the most common bigrams (token pairs) into their own tokens.\n",
        "For example, the most common bigrams from the previous example are \"_n\" and \"on\". Breaking the tie arbitrarily and creating a new token \"_n\" we now have\n",
        "\n",
        "> [W, e, _n, a, m, e, d, _, o, u, r, _, s, o, n, _n, w, o, n, k, u, n]\n",
        "\n",
        "reducing the token count to 22. Iteratively applying this rule, we can further reduce it to 20 tokens by adding the token \"on\", and so on. Each step of this algorithm greedily reduces the token count by the maximum amount.\n",
        "\n",
        "This tokenization scheme, known as \"sub-word tokenization\" takes the best of both worlds: since the vocabulary still contains tokens for every byte, we never have to use the \\<unk\\> token, while still reducing the number of required tokens to encode a sequence. The more tokens you add, the shorter your sequence gets.\n",
        "\n",
        "To decide which tokens to add to the vocabulary, we have to *train* our BPE tokenizer on a corpus.\n",
        "In this section you will do just that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "2AhiO_ZdaHR6"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "train: str = str.join(\" \", dataset[\"train\"][\"text\"])[:pow(10, 6)]\n",
        "test: str = str.join(\" \", dataset[\"test\"][\"text\"])[:pow(10, 6)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQPhCEYWaL5Y",
        "outputId": "ca27826d-cd93-4c2f-a432-ee6e5efde757"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 244/244 [04:00<00:00,  1.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some of our new tokens:\n",
            "'squad '\n",
            "'ha'\n",
            "'her '\n",
            "'batt'\n",
            "'wea'\n",
            "'Ar'\n",
            "'war'\n",
            "'my '\n",
            "'der '\n",
            "'Jap'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from itertools import chain, pairwise\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Tokenizer:\n",
        "    # The lookup list contains *byte groups*, represented as a tuples of ints.\n",
        "    # The token ID for a byte group is its index in the list.\n",
        "    vocab: list[tuple[int, ...]]\n",
        "\n",
        "    def __init__(self, training_seq: str, vocab_size: int) -> None:\n",
        "        # Initialize a lookup with single-byte groups\n",
        "        self.vocab = [(i,) for i in range(pow(2, 8))]\n",
        "        byte_seq = list(bytes(training_seq, \"utf-8\"))\n",
        "        for i in tqdm(range(pow(2, 8), vocab_size)):\n",
        "            \"\"\"\n",
        "            TODO: iteratively add the most common token pairs to the vocabulary.\n",
        "            Advice: try using Counter and pairwise from the python std lib.\n",
        "            \"\"\"\n",
        "            token_seq = self.tokenize(training_seq)\n",
        "            pair_counts = Counter(tuple(pair) for pair in pairwise(token_seq))\n",
        "            if not pair_counts:\n",
        "                break\n",
        "            most_common = max(pair_counts.items(), key=lambda x: x[1])[0]\n",
        "            new_token = tuple(chain.from_iterable(self.vocab[t] for t in most_common))\n",
        "            self.vocab.append(new_token)\n",
        "\n",
        "    def tokenize(self, seq: str) -> list[int]:\n",
        "        \"\"\"\n",
        "        TODO: convert a byte sequence into a token sequence by greedily adding\n",
        "        the longest token that matches the rest of the sequence, e.g.,\n",
        "        vocab = [a, aa, b]\n",
        "        sequence = aaab\n",
        "        token_seq = [1, 0, 2] NOT [0, 1, 2].\n",
        "        \"\"\"\n",
        "        byte_seq: list[int] = list(bytes(seq, \"utf-8\"))\n",
        "        token_seq = []\n",
        "        i = 0\n",
        "        while i < len(byte_seq):\n",
        "            best_len = 1\n",
        "            best_token = self.vocab.index((byte_seq[i],))\n",
        "            for token_id, token in enumerate(self.vocab):\n",
        "                token_len = len(token)\n",
        "                if (i + token_len <= len(byte_seq) and\n",
        "                    tuple(byte_seq[i:i + token_len]) == token and\n",
        "                    token_len > best_len):\n",
        "                    best_len = token_len\n",
        "                    best_token = token_id\n",
        "            token_seq.append(best_token)\n",
        "            i += best_len\n",
        "        return token_seq\n",
        "\n",
        "    def detokenize(self, token_seq: list[int]) -> str:\n",
        "        # TODO: convert a token sequence into a byte sequence.\n",
        "        byte_seq = list(chain.from_iterable(self.vocab[t] for t in token_seq))\n",
        "        return bytes(byte_seq).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "train_data = train[:10000]\n",
        "tokenizer = Tokenizer(train_data, vocab_size=500)\n",
        "\n",
        "print(\"Some of our new tokens:\")\n",
        "for token in tokenizer.vocab[-10:]:\n",
        "    print(repr(bytes(token).decode(\"utf-8\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psiYF10PELk6"
      },
      "source": [
        "As a sanity check, your implementation should be able to compress the training set to ~40-50% of its original size.\n",
        "You should notice that the test set compression does not perform as well. This is because the distribution of bigrams in the test set does not exactly match the that of the train set. This gets worse the further your test set distribution is from your training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osy9s9wiD2A1",
        "outputId": "0d2f2084-6097-4fb6-d720-1cfea300a07f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compressed train set to 41% original size\n",
            "Compressed test set to 52% original size\n"
          ]
        }
      ],
      "source": [
        "# Do not edit this code cell\n",
        "test_data = test[:10000]\n",
        "train_bytes_len = len(bytes(train_data, \"utf-8\"))\n",
        "train_token_len = len(tokenizer.tokenize(train_data))\n",
        "print(f\"Compressed train set to {train_token_len / train_bytes_len * 100:.0f}% original size\")\n",
        "test_bytes_len = len(bytes(test_data, \"utf-8\"))\n",
        "test_token_len = len(tokenizer.tokenize(test_data))\n",
        "print(f\"Compressed test set to {test_token_len / test_bytes_len * 100:.0f}% original size\")\n",
        "\n",
        "assert train_data == tokenizer.detokenize(tokenizer.tokenize(train_data))\n",
        "assert test_data == tokenizer.detokenize(tokenizer.tokenize(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrvZyfOvFqNT"
      },
      "source": [
        "## 1.2 BPE performance on OOD text. (5 pts)\n",
        "\n",
        "Explore how English-trained BPE performs on non-English text by downloading corpora from a few different languages and using your English-trained tokenizer. What do you find? Do the results match your expectations? For what langauges does the tokenizer struggle with the most? How might this impact society if everyone were to use your tokenizer?\n",
        "\n",
        "Include your code, results, and discussion in new cells below.\n",
        "\n",
        "Hint: we recommend you use `load_dataset` to fetch from HuggingFace with `streaming=True` to avoid huge downloads. You might want to take a look at the `oscar` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F_CpvPjHdX8",
        "outputId": "6003327d-5958-489e-b39a-79c8268f55cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average token length for es (sample of 100 sentences): 2370.19\n",
            "Average token length for fr (sample of 100 sentences): 1397.1\n",
            "Average token length for de (sample of 100 sentences): 2153.35\n",
            "Average token length for zh (sample of 100 sentences): 16104.22\n",
            "Results: {'es': 2370.19, 'fr': 1397.1, 'de': 2153.35, 'zh': 16104.22}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "langs = [\"es\", \"fr\", \"de\", \"zh\"]  # Spanish, French, German, Chinese\n",
        "res = defaultdict(list)\n",
        "\n",
        "for lang in langs:\n",
        "    ds = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{lang}\", split=\"train\", streaming=True)\n",
        "    sample_texts = [text[\"text\"] for text in ds.take(100)]\n",
        "    ttl = 0\n",
        "    for sentence in sample_texts:\n",
        "        tokenized = tokenizer.tokenize(sentence)\n",
        "        ttl += len(tokenized)\n",
        "    avg_token_length = ttl / len(sample_texts)\n",
        "    res[lang] = avg_token_length\n",
        "    print(f\"Average token length for {lang} (sample of 100 sentences): {avg_token_length}\")\n",
        "\n",
        "print(\"Results:\", dict(res))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n1MiaDOHNht"
      },
      "source": [
        "\n",
        "In my opinion, the English-trained tokenizer runs pretty well on languages similar to English (e.g. French, Spanish), it suffers a lot on structurally different ones including Chinese. You can tell from the extremely high token counts for chinese, it breaks sentences into many tiny meaningless tokens.\n",
        "\n",
        "I expected these results as the tokenizer is trained on English it seems to perform better on similar script languages. The high token count for Chinese was expected based on its unique script and structure, whereas German continues to be a challenge due mainly to its compound words.\n",
        "\n",
        "I think if this tokenizer was employed for all the languages globally, it would result in inefficiencies and higher processing time + computational costs for non-Latin scripts such as Chinese. It could also instill biases, because the system will optimize for languages similar to English and ultimately limit or misrepresent speakers of other languages. And at the same time, for inclusive AI, we think a more flexible tokenizer is necessary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wc6vsHwKtiC"
      },
      "source": [
        "## 1.3 Pitfalls of and alternatives to BPE (5 pts)\n",
        "\n",
        "BPE tokenization sufferes from other issues as well. Due to the implementation of our BPE tokenizer, detokenizing a sequence of tokens then re-tokenizing it does not always recover the original sequence:\n",
        "```\n",
        "vocab = {a, aa, b}\n",
        "tokens = [0, 1, 2]\n",
        "detokenized = aaab\n",
        "retokenized = [1, 0, 2]\n",
        "```\n",
        "\n",
        "Another issue is that some tokens that may have been prevalent during BPE training may not be present during language model training, leading to funky situations where the language model has not been trained to represent or output some tokens. See this paper for more information: https://arxiv.org/pdf/2405.05417.\n",
        "\n",
        "Some NLP researchers think that we should move away from sub-word tokenization to get rid of these problems. Engage with this discussion by either\n",
        "- Finding a paper that points out an issue with tokenization and propose your own solution for how you would fix it, or\n",
        "- Finding a paper that proposes an alternative tokenization scheme (or way of processing text) and discuss the drawbacks of the proposed method.\n",
        "\n",
        "Your response should be about a paragraph in length and link to a paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY9nKjwwxQAB"
      },
      "source": [
        "I went ahead with  *Finding a paper that proposes an alternative tokenization scheme (or way of processing text) and discuss the drawbacks of the proposed method.*\n",
        "\n",
        "I found a paper named \"Subword Regularization: Improving Neural Network Translation Models\n",
        "with Multiple Subword Candidates\" which proposes a probabilistic tokenization method that competes against the popular byte-pair encoding (BPE). Rather than merging the most common character pairs, it starts off with a huge vocabulary of subwords and does top down through the detail which retains likely candidates based on a uni-gram model whilst pruning others. This approach is relatively more flexible towards different data, and can easily accommodate out-of-vocabulary words.\n",
        "\n",
        "It has the following disadvantages: it is computationally expensive because of probabilistic calculations involved, it is lacking with inflectionally rich languages and humans can also find the segmentations meaningless on unseen words due to potentially sub-optimal segmentations leading to tokens that are less interpretable.\n",
        "\n",
        "Link to the paper: https://arxiv.org/abs/1804.10959"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxgYpTuExIv0"
      },
      "source": [
        "# 2. Generation Algorithms (35 pts + 15 pts BONUS)\n",
        "\n",
        "In this problem, we will implement several common decoding algorithms and test them with the GPT-2 Medium model.\n",
        "\n",
        "Given the class below, we will fill in each of the method stubs. You may create additional helper methods as well to make components re-usable.\n",
        "\n",
        "**You are not allowed to use the generate() function in the transformers library. You can only use the model's forward() method to retrieve final layer logits**\n",
        "\n",
        "In addition to the methods we ask you to implement, which are:\n",
        "- Greedy decoding\n",
        "- Temperature Sampling\n",
        "- Nucleus Sampling\n",
        "\n",
        "You will choose ONE of the following sampling algorithms to implement as well (make sure to add your own method, since we do not provide one by default):\n",
        "- Typical Sampling ([Meister et al. (2022)](https://arxiv.org/abs/2202.00666))\n",
        "- Eta Sampling ([Hewitt et al. (2022)](https://arxiv.org/abs/2210.15191))\n",
        "\n",
        "Points for this question will be distributed as follows:\n",
        "\n",
        "- 5-10 points for implementing each decoding algorithm\n",
        "- 5 points for implementing the generate() function (you will make this incrementally through each sub-part)\n",
        "- 5 points for filling out the table with list of tokens (see instructions below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "BR11jNsRqAw3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from typing import Optional\n",
        "\n",
        "class LM():\n",
        "  def __init__(self, model_name: str = \"openai-community/gpt2-medium\"):\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    self.model.eval()\n",
        "\n",
        "  def greedy_decoding(self, prompt: str, max_length: int = 64) -> str:\n",
        "    \"\"\"\n",
        "    TODO:\n",
        "\n",
        "    Implement greedy decoding, in which we use the highest\n",
        "    probability token at each decoding step\n",
        "    \"\"\"\n",
        "    inp_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    otp_ids = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            otp = self.model(input_ids=inp_ids)\n",
        "            logits = otp.logits[:, -1, :]\n",
        "            nxt_tkn_id = torch.argmax(logits, dim=-1)\n",
        "            otp_ids.append(nxt_tkn_id.item())\n",
        "            inp_ids = torch.cat([inp_ids, nxt_tkn_id.unsqueeze(0)], dim=-1)\n",
        "\n",
        "            if nxt_tkn_id.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    return self.tokenizer.decode(otp_ids, skip_special_tokens=True)\n",
        "\n",
        "  def temperature_sampling(self, prompt: str, temperature: float = 1.0, max_length: int = 64) -> str:\n",
        "    \"\"\"\n",
        "    TODO:\n",
        "\n",
        "    Implement temperature sampling, in which we sample\n",
        "    from the output distribution at each decoding step,\n",
        "    with a temperature parameter to control the \"peakiness\"\n",
        "    of the output distribution\n",
        "    \"\"\"\n",
        "    inp_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    otp_ids = inp_ids[0].tolist()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            otp = self.model(input_ids=inp_ids)\n",
        "            logits = otp.logits[:, -1, :]\n",
        "\n",
        "            if temperature == 0:\n",
        "                nxt_tkn_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "            else:\n",
        "                logits = logits / temperature\n",
        "                probs = torch.softmax(logits, dim=-1)\n",
        "                nxt_tkn_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            otp_ids.append(nxt_tkn_id.item())\n",
        "            inp_ids = torch.cat([inp_ids, nxt_tkn_id], dim=-1)\n",
        "\n",
        "            if nxt_tkn_id.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    return self.tokenizer.decode(otp_ids, skip_special_tokens=True)\n",
        "\n",
        "  def nucleus_sampling(self, prompt: str, p: float = 0.9, max_length: int = 64, temperature: float = 1.0) -> str:\n",
        "    \"\"\"\n",
        "    TODO:\n",
        "    Implement nucleus sampling, in which we\n",
        "    sample from a subset of the vocabulary\n",
        "    at each decoding step\n",
        "    Note: There is also a temperature parameter here\n",
        "    \"\"\"\n",
        "    inp_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    otp_ids = inp_ids[0].tolist()\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        otp = self.model(input_ids=inp_ids)\n",
        "        logits = otp.logits[:, -1, :] / temperature\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        srtd_probs, srtd_idx = torch.sort(probs, descending=True)\n",
        "        cm_probs = torch.cumsum(srtd_probs, dim=-1)\n",
        "        cm_mask = cm_probs <= p\n",
        "        cm_mask[..., 1:] = cm_mask[..., :-1].clone()\n",
        "        cm_mask[..., 0] = True\n",
        "        top_p_probs = srtd_probs[cm_mask]\n",
        "        top_p_idx = srtd_idx[cm_mask]\n",
        "        nxt_tkn_idx = torch.multinomial(top_p_probs, num_samples=1).item()\n",
        "        nxt_tkn_id = top_p_idx[nxt_tkn_idx].item()\n",
        "        otp_ids.append(nxt_tkn_id)\n",
        "        inp_ids = torch.cat([inp_ids, torch.tensor([[nxt_tkn_id]])], dim=-1)\n",
        "\n",
        "        if nxt_tkn_id == self.tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return self.tokenizer.decode(otp_ids, skip_special_tokens=True)\n",
        "\n",
        "  def typical_sampling(self, prompt: str, eta: float = 0.2, max_length: int = 64) -> str:\n",
        "    \"\"\"\n",
        "    TODO:\n",
        "\n",
        "    Implement typical sampling, which focuses on sampling\n",
        "    from tokens that are within a certain \"surprise\" range\n",
        "    based on entropy to prevent unlikely tokens from being chosen.\n",
        "    \"\"\"\n",
        "    inp_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    otp_ids = inp_ids[0].tolist()\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        logits = self.model(input_ids=inp_ids).logits[:, -1, :]\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        srtd_probs, srtd_idx = torch.sort(probs, descending=True)\n",
        "        cm_probs = torch.cumsum(srtd_probs, dim=-1)\n",
        "        typical_mask = cm_probs <= eta\n",
        "        typical_probs = srtd_probs[typical_mask]\n",
        "        typical_idx = srtd_idx[typical_mask]\n",
        "\n",
        "        if typical_probs.numel() == 0:\n",
        "            nxt_tkn_id = srtd_idx.squeeze()[0].item()\n",
        "        else:\n",
        "            nxt_tkn_id = typical_idx[torch.multinomial(typical_probs, num_samples=1)].item()\n",
        "\n",
        "        otp_ids.append(nxt_tkn_id)\n",
        "        inp_ids = torch.cat([inp_ids, torch.tensor([[nxt_tkn_id]])], dim=-1)\n",
        "\n",
        "        if nxt_tkn_id == self.tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return self.tokenizer.decode(otp_ids, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "  def generate(self,\n",
        "               prompt: str,\n",
        "               temperature: float = 1.0,\n",
        "               p: Optional[float] = None) -> str:\n",
        "      \"\"\"\n",
        "      TODO:\n",
        "\n",
        "      Route to the appropriate generation function\n",
        "      based on the arguments\n",
        "      HINT: What parameter values should map to greedy decoding?\n",
        "      \"\"\"\n",
        "      if p is None and temperature == 1.0:\n",
        "        return self.greedy_decoding(prompt)\n",
        "      elif p is not None:\n",
        "        return self.nucleus_sampling(prompt, p=p, temperature=temperature)\n",
        "      else:\n",
        "        return self.temperature_sampling(prompt, temperature=temperature)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ibdqPky8aCXP"
      },
      "outputs": [],
      "source": [
        "GPT2LM = LM(\"openai-community/gpt2-medium\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pJTajqLSq76"
      },
      "source": [
        "For each sampling algorithm you implement, fill out this table, in which you will list the top 10 highest probability tokens **at the first decoding step** in a comma separated list. For algorithms like nucleus sampling where you perform some kind of truncation/re-distribution of the output distribution, do the truncation/re-distribution first, and then sort the vocabulary by probability to complete the table.\n",
        "\n",
        "For this and all questions below, use the following prompt:\n",
        "\n",
        "\n",
        "**\"Once upon a time in a land far far away, \"**\n",
        "\n",
        "Note: Use the default value for `max_length` for all questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZqddssgINXf",
        "outputId": "8464bb03-9242-442f-995b-96a6868b7fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy Decoding Top 10 Tokens:  a young man named   was born\n",
            "Temperature Sampling Top 10 Tokens (t=1.0): Once upon a time in a land far far away, ・ Folklorist (C)\n",
            "\n",
            "──\n",
            "Nucleus Sampling Top 10 Tokens (p=0.9): Once upon a time in a land far far away,  Bonds established an outfit which was really scary\n",
            "Typical Sampling Top 10 Tokens (eta=0.2): Once upon a time in a land far far away,  a man named Tiamat came to the\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time in a land far far away, \"\n",
        "GPT2LM = LM(model_name=\"openai-community/gpt2-medium\")\n",
        "\n",
        "print(\"Greedy Decoding Top 10 Tokens:\", GPT2LM.greedy_decoding(prompt, max_length=10))\n",
        "print(\"Temperature Sampling Top 10 Tokens (t=1.0):\", GPT2LM.temperature_sampling(prompt, temperature=1.0, max_length=10))\n",
        "print(\"Nucleus Sampling Top 10 Tokens (p=0.9):\", GPT2LM.nucleus_sampling(prompt, p=0.9, temperature=1.0, max_length=10))\n",
        "print(\"Typical Sampling Top 10 Tokens (eta=0.2):\", GPT2LM.typical_sampling(prompt, eta=0.2, max_length=10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7tJ9bQoTQ8P"
      },
      "source": [
        "| **Decoding Algorithm** | **10 Highest Probability Tokens** |\n",
        "|------------------------|-----------------------------------|\n",
        "| Greedy                 |   a young man named   was born                                 |\n",
        "| Temperature (t=1.0)    | Once upon a time in a land far far away, ・ Folklorist (C)                                 |\n",
        "| Nucleus (p=0.9)        | Once upon a time in a land far far away,  Bonds established an outfit which was really scary                                 |\n",
        "| Typical/Eta            | Once upon a time in a land far far away,  a man named Tiamat came to the                                 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpV48G_ez8xu"
      },
      "source": [
        "## 2.1 Greedy Decoding (5 points)\n",
        "\n",
        "First, implement the most simple decoding method of greedy decoding. Here, at each decoding time step, simply use the highest probability token. Note that you'll need to adjust the generate function so that a specific temperature value will map to greedy decoding (what should that value be?).\n",
        "\n",
        "Use the prompt given above to test your implementation. What do you notice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RekrTa4IXEW",
        "outputId": "e30b39e6-261b-4885-fe77-1d4b6b2cdea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy Decoding Output: Once upon a time in a land far far away, there lived a man named Simeon. He was a wise man, and he knew the secrets of the universe. He knew that the universe was made of many worlds, and that each world was made of a single substance. He knew that each substance was made of a single substance, and that each substance was made\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time in a land far far away,\"\n",
        "GPT2LM = LM()\n",
        "\n",
        "print(\"Greedy Decoding Output:\", GPT2LM.generate(prompt, temperature=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTuogAKRYFPL"
      },
      "source": [
        "I noticed that by setting the temperature = 0, causes the model to choose the highest probability token at every step, which leads to sensible outputs. However, greedy decoding may produce constant or non-surprising answers since it makes the model choose only top options. This method on the other hand preserves consistency in terms of quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaKx8iVHYZjE"
      },
      "source": [
        "##2.2 Temperature Sampling (10 pts)\n",
        "\n",
        "Sometimes (a lot of the time?), we don't actually just want the highest probability token at each time step. Why might this be the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH_L6QFjYhzY"
      },
      "source": [
        "Unrestrained by always picking the token with the maximum probability, we breathe diversity and creativity into Temperature Sampling. It attempts to avoid generating repetitive or overly tepid text, by drawing from a distribution of plausible tokens. The temperature parameter controls this randomness; a larger temperature will increase diversity and creativity in the outputs, whereas lower temperatures lead to more concentrated and deterministic results. Its tradeoff between coherence and diversity makes temperature sampling a perfect fit for producing provocative and less deterministic text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtwUy0ckIv79"
      },
      "source": [
        "To adjust for this, we often use sampling algorithms instead of greedy decoding. However, there are many ways we can go about sampling.\n",
        "\n",
        "First, implement temperature sampling. Recall that the temperature parameter adjusts the \"randomness\" of the output at each time step. Here, you'll need to think about how to adjust the output distribution which you will do multinomial sampling from. Be careful about how you will handle very low (close to 0) temperatures.\n",
        "\n",
        "Given the same prompt as above, test your implementation with the following temperature values: [0.3, 0.5, 0.7, 0.9, 1.1]. For each value, sample 3 outputs. What do you notice in terms of the differences between output sets across different temperature values?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxI1MULvJppY",
        "outputId": "af14a768-f94c-46b3-e730-d7685dee6159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Temperature: 0.3\n",
            "Output 1: Once upon a time in a land far far away, there lived a king who was fond of many things\n",
            "Output 2: Once upon a time in a land far far away, there was a young man named Sibyl.\n",
            "Output 3: Once upon a time in a land far far away, a young man named Emmett Brown was born\n",
            "\n",
            "Temperature: 0.5\n",
            "Output 1: Once upon a time in a land far far away, a young man was born and raised in a small\n",
            "Output 2: Once upon a time in a land far far away, a princess was born. A princess, who loved\n",
            "Output 3: Once upon a time in a land far far away, there lived a king who lived by his own laws\n",
            "\n",
            "Temperature: 0.7\n",
            "Output 1: Once upon a time in a land far far away, a young warrior named Gob was entrusted with the duty\n",
            "Output 2: Once upon a time in a land far far away, there was a young girl named Lona. She\n",
            "Output 3: Once upon a time in a land far far away, where the sun never sets, and the moon never\n",
            "\n",
            "Temperature: 0.9\n",
            "Output 1: Once upon a time in a land far far away, rumor and legend would blur and fail to exist.\n",
            "Output 2: Once upon a time in a land far far away, in an era far, far apart, old Scout\n",
            "Output 3: Once upon a time in a land far far away, I observed my two strongest brothers, who were young\n",
            "\n",
            "Temperature: 1.1\n",
            "Output 1: Once upon a time in a land far far away, a young princess traveled far and wide with an enchanted\n",
            "Output 2: Once upon a time in a land far far away, valiant Americans touched down here on the islands of O\n",
            "Output 3: Once upon a time in a land far far away, the tale was told. It was tucked away under\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time in a land far far away,\"\n",
        "GPT2LM = LM()\n",
        "temp_vals = [0.3, 0.5, 0.7, 0.9, 1.1]\n",
        "\n",
        "for temp in temp_vals:\n",
        "    print(f\"\\nTemperature: {temp}\")\n",
        "    for i in range(3):\n",
        "        otp = GPT2LM.temperature_sampling(prompt, temperature=temp, max_length=10)\n",
        "        print(f\"Output {i+1}: {otp}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUd4ikx5JrrU"
      },
      "source": [
        "| **Temperature** | **Output 1** | **Output 2** | **Output 3** |\n",
        "|-----------------|--------------|--------------|--------------|\n",
        "| 0.3             | Once upon a time in a land far far away, there lived a king who was fond of many things            |Once upon a time in a land far far away, there was a young man named Sibyl.            |Once upon a time in a land far far away, a young man named Emmett Brown was born            |\n",
        "| 0.5             | Once upon a time in a land far far away, a young man was born and raised in a small            |Once upon a time in a land far far away, a princess was born. A princess, who loved            |Once upon a time in a land far far away, there lived a king who lived by his own laws  |\n",
        "| 0.7             | Once upon a time in a land far far away, a young warrior named Gob was entrusted with the duty            | Once upon a time in a land far far away, there was a young girl named Lona. She            | Once upon a time in a land far far away, where the sun never sets, and the moon never|\n",
        "| 0.9             |Once upon a time in a land far far away, rumor and legend would blur and fail to exist.|Once upon a time in a land far far away, in an era far, far apart, old Scout |Once upon a time in a land far far away, I observed my two strongest brothers, who were young            |\n",
        "| 1.1             |Once upon a time in a land far far away, a young princess traveled far and wide with an enchanted            |Once upon a time in a land far far away, valiant Americans touched down here on the islands of O            |Once upon a time in a land far far away, the tale was told. It was tucked away under            |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zLR0i1YXoGf"
      },
      "source": [
        "I noticed the following differences between output sets across different temperature values:\n",
        "\n",
        "1. Low Temp (0.3, 0.5) — Tend to be more predictable and consistent by following simple patterns\n",
        "\n",
        "2. Middle Temp (0.7): Creativity with balance in coherence — adding a unique piece of information\n",
        "\n",
        "3. High Temp (0.9, 1.1): Best if it should be the highly diverse but more scattered best for random exploration\n",
        "\n",
        "In simple terms, higher temp gives more creative outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-_Y6K1BJwZN"
      },
      "source": [
        "## 2.3 Nucleus Sampling (10 pts)\n",
        "\n",
        "Originally published in [Holtzmann et al. (2021)](https://arxiv.org/abs/1904.09751), nucleus sampling was designed to address an issue that was especially prevalent in language models at the time.\n",
        "\n",
        "This issue is the case of \"neural text degeneration,\" where outputs from LMs would often degenerate into gibberish if a low probability token was ever decoded. To address this, nucleus (also known as top-p) sampling uses a hyperparameter, p, to control how big of a subset of the vocabulary we sample from at each step. For example, if p=0.9, we only sample from the subset of tokens that have a cumulative probability mass of 0.9 (after sorting by probability).\n",
        "\n",
        "Implement nucleus sampling and then use the same prompt as above and test your implementation with the following p-values: [0.97, 0.95, 0.9, 0.8, 0.7]\n",
        "What do you notice across outputs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEbS7DClQ13W",
        "outputId": "8996cc7d-3dc5-4060-a65b-6561dc123c64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Nucleus Sampling (p=0.97)\n",
            "Output 1: Once upon a time in a land far far away, Ȓu I was lucky enough to witness the keys of creation convey the primal spirits and truth of creation. Ȓu was reminded of the absurdity of such crude stargazing when I became contaminated with their false lore about humans, �\n",
            "Output 2: Once upon a time in a land far far away, _____ (seventh ed.), written by Phisate of Navias, but scanned and posted by Daniel Schwartz\n",
            "It has been noticed (March 2008) that every 'Title of Person of Notary' either seems excessively long, hesitates to mention\n",
            "Output 3: Once upon a time in a land far far away,  Campion's predecessor enjoyed a number of sons and daughters married to Clements Hold El. Learning of their fabulations, Clan Evangeline struck ever harder at their kin - forcing them to shrink and adding even more delights to their nursery,\n",
            "\n",
            "Nucleus Sampling (p=0.95)\n",
            "Output 1: Once upon a time in a land far far away,  There lived an  Adventurer who were called by the gods of a maiden named Mors  When they had many adventures.    One day the Adventurer saw a scene they had never seen before of two warriors that each had a\n",
            "Output 2: Once upon a time in a land far far away, _______ lived peacefully.\n",
            "\n",
            "She returned from Sweden once a month. As she wrote a rough version of it to another sister who also had briefly immigrated with her.\n",
            "\n",
            "8th September 1923. The epithet blacks in her body.\n",
            "\n",
            "Output 3: Once upon a time in a land far far away, ******** grew two white grapes for his birthday cake……\n",
            "\n",
            "Although the fact that the space marines were needed under some circumstances made him eat up quite a bit of meat, this dish still had some specialties! ******** wanted his festive treat after all\n",
            "\n",
            "Nucleus Sampling (p=0.9)\n",
            "Output 1: Once upon a time in a land far far away, ichor was something the doctor kept in a pocket, mostly washed down with thick wine.\n",
            "\n",
            "They tried that on Solvanus, who wasn't quite so slender, although he wore it almost daily.\n",
            "\n",
            "On the Fereldan\n",
            "Output 2: Once upon a time in a land far far away, urn of urn of red forever returned to the cavern.\n",
            "\n",
            "This urn of red was the bride of Mundus,\n",
            "\n",
            "It carried gold from the Dwemer city of Vivec\n",
            "\n",
            "She and her attendants slept but one night as\n",
            "Output 3: Once upon a time in a land far far away, 《Fire bird》 released its power and separated into《Alligator》 and《Albino bird》. In reality, 《Alligator》 was a girl named Asha who was holding an �\n",
            "\n",
            "Nucleus Sampling (p=0.8)\n",
            "Output 1: Once upon a time in a land far far away, _________. You think I'm angry? _________. I don't like it when you're angry. _________. You're mad! _________. You think I'm angry? _________. That's because I think you're angry! \n",
            "Output 2: Once upon a time in a land far far away, ikebe wen keepe, ikebe wen come by like a flower. ikebe wen come by, ikebe wen come by like a flower, ikebe wen come by like a flower. \n",
            "Output 3: Once upon a time in a land far far away,  a warrior with a bad temper, a stupid cat and a horny attitude set off to track down the chief's daughter, but now the heroine and the rat killer are on the trail of a young man with a violent streak who turns out to be\n",
            "\n",
            "Nucleus Sampling (p=0.7)\n",
            "Output 1: Once upon a time in a land far far away,  Alicia and her mother were cast out of their home for their mistreatment by the King of Demon Land.   The king decreed that they had to live in a prison.   What did they do?   They ran away from home\n",
            "Output 2: Once upon a time in a land far far away, ichor was filled with deadly poison. A creature was sealed away, like a secret safe, within a piece of magic that could not be opened. Once open, the creature could never be freed. A soul that survived within the tomb would eventually fade\n",
            "Output 3: Once upon a time in a land far far away,  an immortal called the Master resided. He was called by the Great Master of the Gods, \"the master of all\". His name was Sir Hye, and he ruled the earth with his golden light. It was Sir Hye's intention to\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time in a land far far away, \"\n",
        "GPT2LM = LM(model_name=\"openai-community/gpt2-medium\")\n",
        "\n",
        "def test_nucleus_sampling(prompt, p_values):\n",
        "    for p in p_values:\n",
        "        print(f\"\\nNucleus Sampling (p={p})\")\n",
        "        for i in range(1, 4):\n",
        "            print(f\"Output {i}: {GPT2LM.nucleus_sampling(prompt, p=p, max_length=50)}\")\n",
        "p_values = [0.97, 0.95, 0.9, 0.8, 0.7]\n",
        "test_nucleus_sampling(prompt, p_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWKKAdEeVxBG"
      },
      "source": [
        "| **p** | **Output 1** | **Output 2** | **Output 3** |\n",
        "|----------|--------------|--------------|--------------|\n",
        "| 0.97     |Once upon a time in a land far far away, Ȓu I was lucky enough to witness the keys of creation convey the primal spirits and truth of creation. Ȓu was reminded of the absurdity of such crude stargazing when I became contaminated with their false lore about humans, �|Once upon a time in a land far far away, _____ (seventh ed.), written by Phisate of Navias, but scanned and posted by Daniel Schwartz It has been noticed (March 2008) that every 'Title of Person of Notary' either seems excessively long, hesitates to mention | Once upon a time in a land far far away,  Campion's predecessor enjoyed a number of sons and daughters married to Clements Hold El. Learning of their fabulations, Clan Evangeline struck ever harder at their kin - forcing them to shrink and adding even more delights to their nursery,|\n",
        "| 0.95     |Once upon a time in a land far far away,  There lived an  Adventurer who were called by the gods of a maiden named Mors  When they had many adventures.    One day the Adventurer saw a scene they had never seen before of two warriors that each had a|Once upon a time in a land far far away, _______ lived peacefully. She returned from Sweden once a month. As she wrote a rough version of it to another sister who also had briefly immigrated with her 8th September 1923. The epithet blacks in her body.|Once upon a time in a land far far away, ******** grew two white grapes for his birthday cake…… Although the fact that the space marines were needed under some circumstances made him eat up quite a bit of meat, this dish still had some specialties! ******** wanted his festive treat after all|\n",
        "| 0.9      |Once upon a time in a land far far away, ichor was something the doctor kept in a pocket, mostly washed down with thick wine. They tried that on Solvanus, who wasn't quite so slender, although he wore it almost daily. On the Fereldan|Once upon a time in a land far far away, urn of urn of red forever returned to the cavern. This urn of red was the bride of Mundus, It carried gold from the Dwemer city of Vivec She and her attendants slept but one night as|Once upon a time in a land far far away, 《Fire bird》 released its power and separated into《Alligator》 and《Albino bird》. In reality, 《Alligator》 was a girl named Asha who was holding an �|\n",
        "| 0.8      |Once upon a time in a land far far away, _________. You think I'm angry? _________. I don't like it when you're angry. _________. You're mad! _________. You think I'm angry? _________. That's because I think you're angry!|Once upon a time in a land far far away, ikebe wen keepe, ikebe wen come by like a flower. ikebe wen come by, ikebe wen come by like a flower, ikebe wen come by like a flower.|Once upon a time in a land far far away,  a warrior with a bad temper, a stupid cat and a horny attitude set off to track down the chief's daughter, but now the heroine and the rat killer are on the trail of a young man with a violent streak who turns out to be|\n",
        "| 0.7      |Once upon a time in a land far far away,  Alicia and her mother were cast out of their home for their mistreatment by the King of Demon Land.   The king decreed that they had to live in a prison.   What did they do?   They ran away from home|Once upon a time in a land far far away, ichor was filled with deadly poison. A creature was sealed away, like a secret safe, within a piece of magic that could not be opened. Once open, the creature could never be freed. A soul that survived within the tomb would eventually fade|Once upon a time in a land far far away,  an immortal called the Master resided. He was called by the Great Master of the Gods, \"the master of all\". His name was Sir Hye, and he ruled the earth with his golden light. It was Sir Hye's intention to|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42kgPc4UXsHO"
      },
      "source": [
        "I noticed the following across outputs:\n",
        "\n",
        "1. Higher p values tend to increase the diversity and creativity of the output but can lead to degenerative text.\n",
        "\n",
        "2. Lower p values limit token selection to higher-probability options, producing more focused and better outputs with a lower risk of random symbols or unrelated phrases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsIqM7tAQ3U-"
      },
      "source": [
        "## 2.4 More variations on decoding algorithms (10 pts)\n",
        "\n",
        "Nucleus sampling was definitely not the end of the road in terms of new decoding algorithms. Even in the past few years, new decoding algorithms have been proposed to address some limitations of existing algorithms.\n",
        "\n",
        "Two in particular are:\n",
        "- Typical Sampling ([Meister et al. (2022)](https://arxiv.org/abs/2202.00666))\n",
        "- Eta Sampling ([Hewitt et al. (2022)](https://arxiv.org/abs/2210.15191))\n",
        "\n",
        "For this question, CHOOSE ONE of the two algorithms presented above. Below, please describe in a few sentences what your chosen algorithm does in a novel way and the broad motivation behind it. Along with this description, present 3 sampled outputs for the same prompt as above (you can use one hyperparameter value for all of these).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwkhi_7rWZTQ"
      },
      "source": [
        "I choose typical Sampling.\n",
        "\n",
        "This is solution aims to reduce monotony in the text generation from greedy or nucleus sampling and make it more natural as well as human-like, by making the generated sentences likley enough at one moment of time but then switching out for another set of text. While standard methods would cluster around a few high-probability words or treat extremes equally, the typical sampling picks word in accordance with how close it is to expected uncertainty of model predictions.\n",
        "\n",
        "Instead of using just the most probable word, it aims to pick words that fit well in the context of the sentence, which resembles natural human speech. Because it uses words within a range that fit in average uncertainty, typical sampling helps you toward avoiding repetition and the output sounds more natural language-like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "JSKneYh4V2C9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "class LM:\n",
        "    def __init__(self, model_name=\"openai-community/gpt2-medium\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "\n",
        "    def typical_sampling(self, prompt: str, eta: float = 0.2, max_length: int = 64) -> str:\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "        otp_ids = input_ids[0].tolist()\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            otps = self.model(input_ids=input_ids)\n",
        "            logits = otps.logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            srtd_probs, srtd_idx = torch.sort(probs, descending=True)\n",
        "            entropy = -torch.sum(srtd_probs * torch.log(srtd_probs + 1e-9), dim=-1, keepdim=True)\n",
        "            typical_mask = torch.abs(-torch.log(srtd_probs + 1e-9) - entropy) <= eta\n",
        "            typical_probs = srtd_probs[typical_mask]\n",
        "            typical_idx = srtd_idx[typical_mask]\n",
        "\n",
        "            if typical_probs.numel() > 0:\n",
        "                nxt_tkn_idx = torch.multinomial(typical_probs, num_samples=1).item()\n",
        "                nxt_tkn_id = typical_idx[nxt_tkn_idx].item()\n",
        "            else:\n",
        "                nxt_tkn_id = srtd_idx[0, 0].item()\n",
        "\n",
        "            otp_ids.append(nxt_tkn_id)\n",
        "            input_ids = torch.cat([input_ids, torch.tensor([[nxt_tkn_id]])], dim=-1)\n",
        "\n",
        "            if nxt_tkn_id == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        return self.tokenizer.decode(otp_ids, skip_special_tokens=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_ZE8CQBJXqS",
        "outputId": "f1baab89-e693-4acc-cb0b-df1136143403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output 1: Once upon a time in a land far far away, _____ played this ____ music that didn't play with each other but got heard around town: a church bells chorus.\" —http://www.ibiblio.org/sounds/libre.htm\n",
            "\n",
            "Another comment says:\n",
            "\n",
            "\"While watching some opera recently, I stumbled across this picture on Pinterest\n",
            "Output 2: Once upon a time in a land far far away, ichor from all creatures within my presence dried into blood.  There is something terribly, terribly wrong.  What am I, what are we? What will my rebirth do?\n",
            "Output 3: Once upon a time in a land far far away, 【Herakles】 defeated his son, 【Enraged】 defeated his daughter, 【Mystery】 defeated his daughter. Now he wants to rid this world of them all.\" 【Vagrant】 thought back to her encounter with them and a sad thought began to escape her heart. It seemed that\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time in a land far far away, \"\n",
        "GPT2LM = LM()\n",
        "\n",
        "print(\"Output 1:\", GPT2LM.typical_sampling(prompt, eta=0.2))\n",
        "print(\"Output 2:\", GPT2LM.typical_sampling(prompt, eta=0.2))\n",
        "print(\"Output 3:\", GPT2LM.typical_sampling(prompt, eta=0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "020-CZLlSbTJ"
      },
      "source": [
        "| Decoding Method | **Output 1** | **Output 2** | **Output 3** |\n",
        "|-----------------|--------------|--------------|--------------|\n",
        "| Typical Sampling               | Once upon a time in a land far far away, _____ played this ____ music that didn't play with each other but got heard around town: a church bells chorus.\" —http://www.ibiblio.org/sounds/libre.htm Another comment says: \"While watching some opera recently, I stumbled across this picture on Pinterest|Once upon a time in a land far far away, ichor from all creatures within my presence dried into blood.  There is something terribly, terribly wrong.  What am I, what are we? What will my rebirth do?|Once upon a time in a land far far away, 【Herakles】 defeated his son, 【Enraged】 defeated his daughter, 【Mystery】 defeated his daughter. Now he wants to rid this world of them all.\" 【Vagrant】 thought back to her encounter with them and a sad thought began to escape her heart. It seemed that|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iIkmLhfWtEY"
      },
      "source": [
        "## 2.5 BONUS (Up to 15 pts)\n",
        "\n",
        "Can you find a prompt where the continuations do not differ much across multiple sampling strategies, even when we use high temperatures or high p values? (Hint: Think about overfitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNo1xrFyJ1QF"
      },
      "source": [
        "When we use different sampling methods like temperature sampling, nucleus sampling, or typical sampling with high randomness (like high temperature or high p-values), we usually expect the model to produce more varied and creative outputs. However, there are cases where these methods still generate similar responses. This usually happens when the model has a strong association with certain prompts.\n",
        "\n",
        "For example, prompts that ask for common facts or well-known phrases might trigger almost identical answers across sampling methods. The model is so familiar with these prompts that it keeps giving the same response, even when we try to increase randomness.\n",
        "\n",
        "For example: The 1st president of India was..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kweZjWF3LpfT"
      },
      "source": [
        "# 3. Prompting (50 pts)\n",
        "\n",
        "In this problem, we will try various prompting approaches and prompt an LLM for a Math Reasoning Benchmark called [GSM8K](https://github.com/openai/grade-school-math), which contains grade school math word problems. This is a very common _reasoning_ benchmark used to test various LLMs.\n",
        "\n",
        "The LLM that we will be using is [Google Gemini](https://gemini.google.com/). We will be prompting Gemini by using an API call to the Gemini Model. Normally, you can also prompt Open Source LLMs via the HuggingFace Library, however due to compute constraints, we use Gemini in this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56o8lVgVNVXq"
      },
      "source": [
        "## Setting up the GSM8K Dataset and Google Gemini\n",
        "\n",
        "Follow the steps below to download the GSM8K Dataset and to setup Google Gemini on Colab. You will automatically get points for this subpr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Cg_sNCLeL5j0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"gsm8k\", 'main')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcI6_53zN5if",
        "outputId": "f54e4239-0794-4b61-ab73-37dc5e4f22e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7473, 1319)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset['train']), len(dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zfGgqwZN9EN",
        "outputId": "1250e7c2-e636-4d8d-8775-5185d849e036"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. How many sheep do Toulouse, Charleston, and Seattle have together if Seattle has 20 sheep?',\n",
              " 'answer': 'If Seattle has 20 sheep, Charleston has 4 * 20 sheep = <<20*4=80>>80 sheep\\nToulouse has twice as many sheep as Charleston, which is 2 * 80 sheep = <<2*80=160>>160 sheep\\nTogether, the three has 20 sheep + 160 sheep + 80 sheep = <<20+160+80=260>>260 sheep\\n#### 260'}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# An example instance of this dataset\n",
        "\n",
        "dataset['test'][6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY-vwQI5OmPJ"
      },
      "source": [
        "### Gemini Setup (from the official [Gemini documentation](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/get-started/python.ipynb))\n",
        "\n",
        "\n",
        "Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.\n",
        "\n",
        "<a class=\"button button-primary\" href=\"https://makersuite.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">Get an API key</a>\n",
        "\n",
        "In Colab, add the key to the secrets manager under the \"🔑\" in the left panel.\n",
        "\n",
        "---\n",
        "\n",
        "Give it the name `GEMINI_API_KEY`.\n",
        "\n",
        "Once you have the API key, pass it to the SDK. You can do this in two ways:\n",
        "\n",
        "* Put the key in the `GEMINI_API_KEY` environment variable (the SDK will automatically pick it up from there).\n",
        "* Pass the key to `genai.configure(api_key=...)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GLn0kVMAQ8u",
        "outputId": "eda88670-1299-445a-9535-35b56c461a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.151.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.25.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.23.4)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.67.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "QlcHWlpQOIh6"
      },
      "outputs": [],
      "source": [
        "# All imports for this question\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from datasets import Dataset\n",
        "import random\n",
        "from typing import Callable, List, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "YZOA3PVIPWxD"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Cv8B6I2lPiMw",
        "outputId": "caee01a6-568a-4c84-d1a5-57d8db0978a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imagine you have a special friend named Alexa or Siri who can understand what you say and answer your questions. Natural Language Processing (NLP) is like giving Alexa or Siri superpowers! It's like a secret code that computers use to chat with us in a way that we can understand, just like you chat with your friends. It's like having a translator who helps the computer understand us better. When we talk to the computer, NLP translates our words into a language the computer can understand. It's like a super-smart assistant that helps Alexa and Siri understand us and give us the best answers they can!\n"
          ]
        }
      ],
      "source": [
        "# Test if your setup is working, do not change the model name\n",
        "model = genai.GenerativeModel(\"gemini-1.0-pro\")\n",
        "response = model.generate_content(\"What is Natural Language Processing? Explain it to a five year old.\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAAyTvXeQ5jD"
      },
      "source": [
        "## 3.1 Data and Prompting Setup (15 + 5 pts)\n",
        "\n",
        "In this part, we will create some boilerplate code to process our dataset and generate prompts from the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4njT8xp2apCu"
      },
      "source": [
        "### Processing the GSM8K Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "6IDGGocfQldW"
      },
      "outputs": [],
      "source": [
        "def process_gsm8k_answers(dataset: Dataset) -> Dataset:\n",
        "    \"\"\"\n",
        "    Processes the GSM8K dataset to remove reasoning chains and retain only the numerical answers.\n",
        "    Assumes answers are separated from reasoning by the '###' string.\n",
        "\n",
        "    Args:\n",
        "    dataset (Dataset): Huggingface Dataset object for GSM8K.\n",
        "\n",
        "    Returns:\n",
        "    Dataset: Processed Dataset object with numerical answers only.\n",
        "    \"\"\"\n",
        "\n",
        "    def extract_answer(sample):\n",
        "        # IMPLEMENT HERE\n",
        "        # Split the answer using '###' and return a dictionary with the key 'processed_answer'\n",
        "        ans = sample['answer'].split('###')\n",
        "        processed_answer = ans[-1].strip()\n",
        "        return {'processed_answer': processed_answer}\n",
        "\n",
        "\n",
        "    return dataset.map(extract_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ddnQ6b4azH7"
      },
      "source": [
        "### Building Prompts (15 pts)\n",
        "\n",
        "We will be implementing FIVE (5) prompting methods. See their descriptions below -\n",
        "1. **Zero-Shot Answer Only (2 pts)**: You prompt the model to only generate the answer to the question\n",
        "\n",
        "2. **Zero-Shot Chain of Thought (CoT) (3 pts)**: Refer to the [Chain of Thought Paper](https://arxiv.org/abs/2201.11903). CoT refers to a reasoning chain that is generated by the model before generating the actual answer. This has shown to improve performance. In this setup, you will prompt the model to generate a reasoning chain before the answer.\n",
        "\n",
        "3. **5-Shot Answer Only (2 pts)**: You provide some in-context examples to prompt the model with to generate the answer. This is analogous to Approach 1. Use the a random set of 5 examples from the training set to create the in-context examples.\n",
        "\n",
        "4. **5-Shot CoT (3 pts)**: Combine Approaches 2 and 3 to do 5-shot CoT prompting.\n",
        "\n",
        "5. **Your own prompt! (5 pts)**: Try something new. Think about how you solve Math problems and implement your own prompting method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "MiW-lHk1aniq"
      },
      "outputs": [],
      "source": [
        "def prompt_generation_zero_shot(problem: str) -> str:\n",
        "    \"\"\"\n",
        "    Zero-shot prompt.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt.\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "    return f\"Answer this math problem: {problem}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "8gSts3jebOLp"
      },
      "outputs": [],
      "source": [
        "def prompt_generation_zero_shot_cot(problem: str) -> str:\n",
        "    \"\"\"\n",
        "    Zero-shot Chain of Thought (CoT) prompt.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt.\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "    return f\"Solve this math problem step-by-step and then provide the final answer:\\n\\nProblem: {problem}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "G1RQZKU-bbi4"
      },
      "outputs": [],
      "source": [
        "def prompt_generation_5_shot(problem: str, training_set: Dataset) -> str:\n",
        "    \"\"\"\n",
        "    5-shot prompt generation for GSM8K problems. Randomly selects 5 examples from the training set.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt with 5 in-context_examples.\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "    exs = random.sample(list(training_set), 5)\n",
        "    in_cntxt_ex = \"\"\n",
        "\n",
        "    for example in exs:\n",
        "      in_cntxt_ex += f\"Problem: {example['question']}\\nAnswer: {example['processed_answer']}\\n\\n\"\n",
        "\n",
        "    prompt = f\"{in_cntxt_ex}Problem: {problem}\\nAnswer:\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "UNteVGTWbnLW"
      },
      "outputs": [],
      "source": [
        "def prompt_generation_5_shot_cot(problem: str, training_set: Dataset) -> str:\n",
        "    \"\"\"\n",
        "    5-shot Chain of Thought (CoT) prompt generation. Randomly selects 5 examples\n",
        "    from the training set and includes reasoning steps.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt with 5 CoT in-context examples.\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "    exs = random.sample(list(training_set), 5)\n",
        "    in_cntxt_ex = \"\"\n",
        "\n",
        "    for example in exs:\n",
        "      in_cntxt_ex += f\"Problem: {example['question']}\\nSolution: {example['answer']}\\n\\n\"\n",
        "\n",
        "    prompt = f\"{in_cntxt_ex}Problem: {problem}\\nSolution:\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "8OhEcDetbP3U"
      },
      "outputs": [],
      "source": [
        "# Feel free to change the method definition\n",
        "\n",
        "def my_prompt(problem: str) -> str:\n",
        "    \"\"\"\n",
        "    Your own unique way of prompting an LLM for Math word problems.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "    return (\n",
        "        f\"Please solve the following math problem step-by-step, and double-check your final answer.\\n\\n\"\n",
        "        f\"Problem: {problem}\\n\"\n",
        "        f\"Solution (with detailed reasoning):\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4uARh18cXDH"
      },
      "source": [
        "## 3.2 Prompting Gemini and Implementing Self-Consistency (5 + 5 + 10 pts)\n",
        "\n",
        "Here, you will help build the wrapper for prompting Gemini using the prompt methods you have designed above.\n",
        "\n",
        "You will then also implement Self-Consistency based prompting. Refer to the [Self-Consistency Paper](https://arxiv.org/abs/2203.11171). In order to implement Self-Consistency, you generate multiple Zero-Shot CoT (Approach 2 in the prompting methods) candidates, and take a majority vote of the answers predicted by each candidate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuVhq91Oc5TB"
      },
      "source": [
        "### First, write the function where you will process the answer generated by the model. (5 pts)\n",
        "\n",
        "Note that answer processing changes for different prompt types, so this function also takes in the name of the method in its argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "TDZwuVpeUCyl"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def answer_processing(prediction: str, prompt_function: Any) -> str:\n",
        "    \"\"\"\n",
        "    Processes the model's generated output to extract the final answer.\n",
        "\n",
        "    Returns:\n",
        "    str: The processed numerical answer.\n",
        "    \"\"\"\n",
        "    prompt_name = prompt_function.__name__\n",
        "    new_pred = re.sub(r'[^\\d\\s.]', '', prediction)\n",
        "    ans = re.findall(r'\\d+\\.?\\d*', new_pred)\n",
        "\n",
        "    if ans:\n",
        "        if prompt_name in [\"prompt_generation_zero_shot\", \"prompt_generation_5_shot\"]:\n",
        "            return ans[-1]\n",
        "        elif prompt_name in [\"prompt_generation_zero_shot_cot\", \"prompt_generation_5_shot_cot\", \"my_prompt\"]:\n",
        "            largest_answer = max(map(float, ans))\n",
        "            return str(int(largest_answer)) if largest_answer.is_integer() else str(largest_answer)\n",
        "    return \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "hWOHGuWzUF2Z"
      },
      "outputs": [],
      "source": [
        "# Do not change, method to calculate accuracy from predictions and ground truth labels\n",
        "\n",
        "def evaluate_accuracy(predictions: List[str], ground_truths: List[str]) -> float:\n",
        "    correct = 0\n",
        "    total = len(predictions)\n",
        "\n",
        "    for pred, true in zip(predictions, ground_truths):\n",
        "        standardized_true = true.lstrip('# ').strip()\n",
        "        if pred == standardized_true:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXg7blmfZeJ"
      },
      "source": [
        "### Next, write the wrapper function where you use all the building blocks constructed above to prompt the Gemini model (5 + 10 pts)\n",
        "\n",
        "\n",
        "On how to prompt Gemini, refer to the [Gemini Text Generation Handbook](https://ai.google.dev/gemini-api/docs/text-generation?lang=python).\n",
        "\n",
        "Hint: Reading this will help you figure out how to generate multiple candidates to implement Self-Consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "pqbrtOXjwe17"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from typing import Callable, List, Any\n",
        "\n",
        "def prompt_gemini_with_self_consistency(\n",
        "    problem: str,\n",
        "    prompt_function: Callable[[str], str],\n",
        "    model_instance: Any,\n",
        "    num_candidates: int\n",
        ") -> str:\n",
        "\n",
        "    pos_ans = []\n",
        "    for _ in range(num_candidates):\n",
        "        if prompt_function in [prompt_generation_5_shot, prompt_generation_5_shot_cot]:\n",
        "            prompt = prompt_function(problem, training_set=None)\n",
        "        else:\n",
        "            prompt = prompt_function(problem)\n",
        "        res = model_instance.generate_content(prompt)\n",
        "        ans = answer_processing(res.text.strip(), prompt_function)\n",
        "        pos_ans.append(ans)\n",
        "    if pos_ans:\n",
        "        final_ans = Counter(pos_ans).most_common(1)[0][0]\n",
        "        return final_ans\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Y0JOy65dTNJ4"
      },
      "outputs": [],
      "source": [
        "def pipeline_generate(\n",
        "    model_instance: Any,\n",
        "    test_set: Dataset,\n",
        "    prompt_function: Callable[[str], str],\n",
        "    process_answer_function: Callable[[str], str],\n",
        "    evaluation_function: Callable[[List[str], List[str]], float],\n",
        "    self_consistency: int,\n",
        "    training_set: Dataset = None\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    model_instance (Any): The Google Gemini model instance.\n",
        "    test_set (Dataset): The GSM8K test set to evaluate on.\n",
        "    prompt_function (Callable): Function to generate prompts for the test set.\n",
        "    process_answer_function (Callable): Function to process the model's generated answers.\n",
        "    evaluation_function (Callable): Function to evaluate model's answers against the ground truth.\n",
        "    self_consistency: Number of samples to run self-consistency approach on.\n",
        "    If negative, 0 or 1, this implies regular prompting.\n",
        "\n",
        "    Returns:\n",
        "    float: The accuracy of the model on the test set.\n",
        "    \"\"\"\n",
        "    preds = []\n",
        "    ground_truths = []\n",
        "\n",
        "    for sample in test_set:\n",
        "        problem = sample['question']\n",
        "        ground_truth = sample['processed_answer']\n",
        "        ground_truths.append(ground_truth)\n",
        "\n",
        "        if self_consistency > 1:\n",
        "            final_answer = prompt_gemini_with_self_consistency(\n",
        "                problem, prompt_function, model_instance, num_candidates=self_consistency\n",
        "            )\n",
        "        else:\n",
        "            if prompt_function in [prompt_generation_5_shot, prompt_generation_5_shot_cot]:\n",
        "                prompt = prompt_function(problem, training_set=training_set)\n",
        "            else:\n",
        "                prompt = prompt_function(problem)\n",
        "\n",
        "            response = model_instance.generate_content(prompt)\n",
        "            prediction = response.text.strip()\n",
        "            final_answer = process_answer_function(prediction, prompt_function)\n",
        "        preds.append(final_answer)\n",
        "\n",
        "    accuracy = evaluation_function(preds, ground_truths)\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "tzHoeMnyUIYp",
        "outputId": "5c22a3a6-89e2-4e90-cf96-638cd0301cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 60.0%\n"
          ]
        }
      ],
      "source": [
        "gsm8k_test_processed = process_gsm8k_answers(dataset['test'])\n",
        "gsm8k_training_set = process_gsm8k_answers(dataset['train'])\n",
        "# The following line is just to test your systems, comment this line out to report results on the entire\n",
        "gsm8k_test_processed = Dataset.from_dict(gsm8k_test_processed[:5])\n",
        "\n",
        "# Run model generation with zero-shot prompt generation\n",
        "accuracy = pipeline_generate(\n",
        "    model_instance=model,\n",
        "    test_set=gsm8k_test_processed,\n",
        "    prompt_function=prompt_generation_zero_shot_cot, # Change this to test different prompt methods\n",
        "    process_answer_function=answer_processing,\n",
        "    evaluation_function=evaluate_accuracy,\n",
        "    self_consistency=2,\n",
        "    training_set=gsm8k_training_set\n",
        ")\n",
        "\n",
        "print(f\"Accuracy: {accuracy}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPt0dz1lgpZS"
      },
      "source": [
        "## 3.3 Complete this table based on your implementation in 3.2 and answer the following questions (5 + 5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rSganJ6gz8P"
      },
      "source": [
        "### Round each value up to two decimal points (5 pts)\n",
        "\n",
        "Method|Accuracy\n",
        "---|---|\n",
        "0-shot| 80%\n",
        "0-shot CoT| 60%\n",
        "5-shot| 80%\n",
        "5-shot CoT| 20%\n",
        "My prompt| 40%\n",
        "0-shot CoT Self-Consistency| 60%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvW7VTADhLa9"
      },
      "source": [
        "### What was the intuition behind the prompt that you designed? (2 pts)\n",
        "\n",
        "The intuition behind the prompt was to make the model go through a careful solution step-by-step, prompting the model to think about how it would solve this problem before providing its answer. This essentially tells the model to double-check its original response, which can help minimize error and improve accuracy especially for multi-step problems or complex calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSknWcWrhedg"
      },
      "source": [
        "### What are the merits and demerits of using advanced prompting approaches like Chain of Thought or Self-Consistency? (3 pts)\n",
        "\n",
        "**Merits:**\n",
        "\n",
        " 1.These COT promptings encourage the model to think through every step in a solution, the reasoning often leads to increased accuracy for more complicated problems.\n",
        "\n",
        "2. These Self Consistency approach outputs a solution, and you present it with multiple answers, it will pick whichever answer came out as the most common, and this reduces random errors.\n",
        "\n",
        "3. These methods work well for multi-step or reasoning-heavy tasks, since simple answers may be less accurate.\n",
        "\n",
        "**Demerits:**\n",
        "\n",
        "1. They might generate many outputs that can be computationally costly and time-consuming.\n",
        "\n",
        "2. Such prompting methods usually need a low-level design and tuning of parameters, which can lead to increased complexity in implementation.\n",
        "\n",
        "3. These methods won’t bring large accuracy benefits to very simple questions, and they are strong for such scenerios."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
